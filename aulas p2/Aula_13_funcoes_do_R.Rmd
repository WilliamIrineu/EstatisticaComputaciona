---
title: "Aula 13: Otimização numérica - Aspectos Computacionais"
author: "Prof. Dr. Eder Angelo Milani"
date: "19/06/2023"
output:
  html_document:
    df_print: paged
---



A seguir é apresentado algumas funções interessantes no R que podem ajudar na difícil tarefa de otimização de funções. 

# Funções hessian e grad no R

O método de Newton-Raphson, que além do gradiente, usa o hessiano e é apropriado para calcular estimadores de máxima verossimilhança por meio de uma aproximação quadrática da verossimilhança ao redor de valores iniciais dos parâmetros, apresenta a dificuldade dos cálculos teóricos.  

Uma alternativa é a obtenção dos valores necessários por meio de aproximações numéricas. Veja o exemplo a seguir. 


## Exemplo 1 

Voltando ao Exemplo 2 da Aula 10, foi gerado uma amostra da distribuição Weibull($\alpha, \gamma$), na sequência foi obtido as funções de verossimilhança, log-verossimilhança, vetor escore e matriz hessiana. 

Podemos utilizar funções no R que calculam, de forma aproximada, o vetor escore e a matriz hessiana. Portanto, facilitando o processo como um todo. 


```{r}
set.seed(2023)
n <- 1000
alpha <- 2
gama <- 0.5

x<- rweibull(n, shape =gama , scale = alpha)

summary(x)

# o valor esperado e
alpha*gamma(1+1/gama)

# construir a funcao objeto 

log_vero <- function(p0){
  alpha <- p0[1]
  gama <- p0[2]
  n <- length(x)
  aux <- n*log(gama) - n*gama*log(alpha) + (gama-1)*sum(log(x)) - sum((x/alpha)^gama)
  return(aux)
}

# teste da funcao 
log_vero(p0=c(1,0.5))


# pacote a ser carregado
library(numDeriv)

# o vetor escore 
grad(log_vero, x=c(1,0.5))

# a matriz hessian
hessian(log_vero, x=c(1,0.5))


# metodo de newton-raphson

n.max <- 100
theta <- matrix(NA, nrow=2, ncol=n.max)
dif_ <- 1
epsilon <- 0.001
cont <- 1
theta[, 1] <- c(1, 2)

while(abs(dif_)>= epsilon & cont<=n.max){
  cat("cont+1=", cont+1, "\n")
  theta[ ,cont+1] <- theta[ ,cont] - solve(hessian(log_vero, theta[ ,cont]))%*%grad(log_vero, theta[ ,cont])
  cat("theta[", cont+1, "]=", theta[ ,cont+1], "\n")
  dif_ <- sqrt(sum((theta[ ,cont+1]-theta[ ,cont])^2))
  cat("dif=", dif_, "\n")
  cont <- cont+1
}

cont

theta[, 1:cont]

```

Compare os resultados obtidos aqui com os resultados da Aula 10.



# Funções optim e MaxLik 

Para obter máximos ou mínimos de funções pode-se usar a função `optim()` do pacote `stats`, que inclui vários métodos de otimização, dentre os quais destacamos:

i) o método default que é o Método Simplex de Nelder e Mead (1965), e corresponde a um método de busca não gradiente,

ii) o método BFGS,

iii) o método do gradiente conjugado (conjugate gradient - CG), 

iv) o método L-BFGS-B, que permite restrições limitadas, 

v) o método SANN, que é método não gradiente e pode ser encarado como uma variante do método da têmpera simulada (simulated annealing). 


Outra opção é o pacote `optimization`, que também inclui vários métodos, como o método Nelder-Mead e da têmpera simulada. O pacote `maxLik` também é uma opção, contendo quase todas as funções do `stats`, além do algoritmo de `Newton–Rapson` (função `maxNR()`). Esse pacote é apropriado para a maximização de verossimilhanças, daí o rótulo maxLik.


## Exemplo 2 

Considere a função $f(x,y)=\exp(-(x+ y))$, que tem um máximo no ponto (0, 0) e valor máximo igual a 1. A seguir é apresentado a função `maxNR()` para obter o ponto de máximo. 

```{r}
#install.packages("maxLik")
library("maxLik")

f1 <- function(theta){ 
x <- theta[1]
y <- theta[2]
z <- exp(-(x^2+y^2))
return(z)
}

result <- maxNR(f1, start=c(1,1))

print(summary(result))
```

## Exemplo 3 

Uma função comumente usada como teste em otimização é a função de Rosenbrock, ou função banana

$$f(x,y)=(1-x)^2+100(y-x^2)^2,$$

Essa função tem um mínimo global em (1, 1). Os resultados da aplicação da função `optim()` para determinar o mínimo dessa função, usando diferentes métodos são apresentados a seguir.


```{r}
f1 <- function(theta){ 
x <- theta[1]
y <- theta[2]
#cat("x=", x, "y=", y, "\n")
z <- (1-x)^2 + 100*(y-x^2)^2
return(z)
}

chute <- c(-1.2, 1)

# BFGS

optim(chute, f1, method="BFGS")

# L-BFGS-B

optim(chute, f1, method="L-BFGS-B")

# CG

optim(chute, f1, method="CG")

# Nelder-Mead

optim(chute, f1, method="Nelder-Mead")

```


Exercícios: 

**(Entregar na sala de aula)** Gerar 100 valores da distribuição normal($\mu=3, \sigma^2=4$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha as estimativas de máxima verossimilhança dos parâmetros $\mu$ e $\sigma^2$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.


(1) Gerar 100 valores da distribuição Poisson($\lambda=3$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha a estimativa de máxima verossimilhança do parâmetro $\lambda$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.


(2) Gerar 300 valores da distribuição Bernoulli($p=0,3$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha a estimativa de máxima verossimilhança do parâmetro $p$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.


(3) Gerar 100 valores da distribuição Exp($\lambda=0.3$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha a estimativa de máxima verossimilhança do parâmetro $\lambda$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.


(4) Gerar 100 valores da distribuição gama($\alpha=10, \beta=10$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha as estimativas de máxima verossimilhança dos parâmetros $\alpha$ e $\beta$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.
 
 
(5) Gerar 100 valores da distribuição beta($\alpha=10, \beta=10$). Utilizando o método de Newton-Rapson, como feito anteriormente, obtenha as estimativas de máxima verossimilhança dos parâmetros $\alpha$ e $\beta$. Obtenha também a estimativa adotando a função *optim* e compare os resultados.
  
